{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "b2HdAoMAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Yuxiang Lin", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=b2HdAoMAAAAJ&citpid=1", "affiliation": "Georgia Tech", "organization": 15204255840385501358, "interests": ["Multimodal Learning", "Large Language Model", "Foundation Model"], "email_domain": "@gatech.edu", "homepage": "https://lum1104.github.io/", "citedby": 58, "publications": {"b2HdAoMAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning", "pub_year": "2024"}, "filled": false, "author_pub_id": "b2HdAoMAAAAJ:qjMakFHDy7sC", "num_citations": 32, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2448494290607946493", "cites_id": ["2448494290607946493"]}, "b2HdAoMAAAAJ:u-x6o8ySG0sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Semi-supervised multimodal emotion recognition with expression mae", "pub_year": "2023"}, "filled": false, "author_pub_id": "b2HdAoMAAAAJ:u-x6o8ySG0sC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4939273979376975490", "cites_id": ["4939273979376975490"]}, "b2HdAoMAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models", "pub_year": "2024"}, "filled": false, "author_pub_id": "b2HdAoMAAAAJ:2osOgNQ5qMEC", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5236914371299119484", "cites_id": ["5236914371299119484"]}, "b2HdAoMAAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Invisible gas detection: An RGB-thermal cross attention network and a new benchmark", "pub_year": "2024"}, "filled": false, "author_pub_id": "b2HdAoMAAAAJ:zYLM7Y9cAGgC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11991209724720340286", "cites_id": ["11991209724720340286"]}, "b2HdAoMAAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing", "pub_year": "2024"}, "filled": false, "author_pub_id": "b2HdAoMAAAAJ:UeHWp8X0CEIC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5125940474790820191", "cites_id": ["5125940474790820191"]}, "b2HdAoMAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models", "pub_year": "2025"}, "filled": false, "author_pub_id": "b2HdAoMAAAAJ:YsMSGLbcyi4C", "num_citations": 0}}, "citedby5y": 58, "hindex": 3, "hindex5y": 3, "i10index": 2, "i10index5y": 2, "cites_per_year": {"2024": 31, "2025": 27}, "updated": "2025-05-12 08:26:01.232545"}